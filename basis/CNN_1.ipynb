{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些注意点：\n",
    "- padding的方式分为两种，'valid'和'same'\n",
    "- valid采用丢弃的方式，多余元素去掉\n",
    "- same采用补全的方式，按左奇右偶的方式补0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 不打印 warning \n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(10000, 10)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "#用tensorflow 导入数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./data/MNIST_data', one_hot = True)\n",
    "print(mnist.test.labels.shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = tf.placeholder(tf.float32, [None, 784])\n",
    "y_input = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#把x转化为卷积所需要的形式\n",
    "X = tf.reshape(X_input, [-1, 28, 28, 1])\n",
    "#第一层卷积:5*5*1卷积核32个[5,5,1,32]\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "#第一个pooling层\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#第二层卷积:5*5*32卷积核64个[5,5,32,64]\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = weight_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "#fc1\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 =bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#dropout 输出维度和h_fc1一样只是随机部分值为0\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)\n",
    "\n",
    "#输出层\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_input: Tensor(\"Placeholder:0\", shape=(?, 784), dtype=float32)\n",
      "X:       Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "y_input: Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32)\n",
      "h_conv1: Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "h_pool1: Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "h_conv2: Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "h_pool2: Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "h_fc1:   Tensor(\"Relu_2:0\", shape=(?, 1024), dtype=float32)\n",
      "y_pred:  Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('X_input:', X_input)\n",
    "print('X:      ', X)\n",
    "print('y_input:', y_input)\n",
    "print('h_conv1:', h_conv1)\n",
    "print('h_pool1:', h_pool1)\n",
    "print('h_conv2:', h_conv2)\n",
    "print('h_pool2:', h_pool2)\n",
    "print('h_fc1:  ', h_fc1)\n",
    "print('y_pred: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.训练和预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, train cost=15.128507, acc=0.950000; test cost=1547.071533, acc=0.955800\n",
      "step 1000, train cost=15.411701, acc=0.950000; test cost=958.668884, acc=0.970000\n",
      "step 1500, train cost=12.403200, acc=0.970000; test cost=774.611023, acc=0.975700\n",
      "step 2000, train cost=4.158594, acc=0.980000; test cost=591.042786, acc=0.980800\n",
      "step 2500, train cost=13.705905, acc=0.940000; test cost=496.175629, acc=0.983200\n",
      "step 3000, train cost=5.975717, acc=0.990000; test cost=442.569336, acc=0.985300\n",
      "step 3500, train cost=6.144988, acc=0.980000; test cost=420.727844, acc=0.986600\n",
      "step 4000, train cost=9.482490, acc=0.990000; test cost=377.285553, acc=0.986900\n",
      "step 4500, train cost=3.740895, acc=0.990000; test cost=348.553772, acc=0.988300\n",
      "step 5000, train cost=11.455607, acc=0.980000; test cost=326.203430, acc=0.988800\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_input *tf.log(y_pred))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(5000):\n",
    "    X_batch, y_batch = mnist.train.next_batch(batch_size=100)\n",
    "    cost, acc, _ =sess.run([cross_entropy, accuracy, train_step], feed_dict={X_input:X_batch, y_input:y_batch, keep_prob:0.5})\n",
    "    if (i+1) % 500 == 0:\n",
    "        test_cost, test_acc = sess.run([cross_entropy, accuracy], feed_dict={X_input: mnist.test.images, y_input: mnist.test.labels, keep_prob: 1.0})\n",
    "        print(\"step {}, train cost={:.6f}, acc={:.6f}; test cost={:.6f}, acc={:.6f}\".format(i+1, cost, acc, test_cost, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
